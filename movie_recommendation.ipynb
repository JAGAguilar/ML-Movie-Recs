{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block One: First Filtering\n",
    "#reading the CSV files, change them on your personal computer to wherever you saved them\n",
    "rate = pd.read_csv(r'ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv(r'ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv(r'ml-latest-small/ratings.csv')\n",
    "tags = pd.read_csv(r'ml-latest-small/tags.csv')\n",
    "userID = ratings\n",
    "#dropping timestamp as it's unnecessary\n",
    "tags = tags.drop(columns='timestamp')\n",
    "ratings = ratings.drop(columns='timestamp') \n",
    "\n",
    "#Groups on userId and movieId and groups tags into a list\n",
    "tags = tags.groupby(['movieId']).agg({'tag':set}).reset_index()\n",
    "\n",
    "#Refining ratings to just be movieId and the average rating to only 1 decimal point\n",
    "ratings = ratings.groupby('movieId')['rating'].mean().reset_index()\n",
    "ratings = ratings.rename(columns={'rating':'average_rating'})\n",
    "ratings['average_rating'] = ratings['average_rating'].round(1)\n",
    "filtered_data = pd.merge(movies,ratings, on='movieId')\n",
    "filtered_data = pd.merge(filtered_data, tags, on='movieId')\n",
    "\n",
    "#Block Two: Unique Genres and Second Filtering\n",
    "# Split genres by '|' and get all unique genres\n",
    "unique_genres = set()\n",
    "for genres in movies['genres']:\n",
    "    unique_genres.update(genres.split('|'))\n",
    "unique_genres = sorted(unique_genres)  # Sort in alphabetic order\n",
    "for genre in unique_genres:\n",
    "    filtered_data[genre] = filtered_data['genres'].apply(lambda x: 1 if genre in x.split('|') else 0)\n",
    "#filter out In Netflix Queue\n",
    "filtered_data['tag'] = filtered_data['tag'].apply(lambda tag_set: {tag for tag in tag_set if tag != \"In Netflix queue\"})\n",
    "\n",
    "# Drop the original 'genres' column (I think it's a good idea, no need to clutter the data yk?)\n",
    "filtered_data= filtered_data.drop(columns=['genres'])\n",
    "\n",
    "\n",
    "#Block Three: Vectorization\n",
    "#I changed the vector size to 10 just cause it's smaller but if you wanna increase it by all means go for it\n",
    "wordVector = Word2Vec(filtered_data['tag'].tolist(),vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "vector_size = 10  # Same size as Word2Vec vectors\n",
    "padding_vector = np.zeros(vector_size)\n",
    "# Function to generate a vector for up to 3 tags\n",
    "def create_feature_vector(row):\n",
    "    # Get word vectors for the tags, up to 3 tags, and pad if fewer\n",
    "    tag_vectors = [\n",
    "        wordVector.wv[tag] if tag in wordVector.wv else padding_vector\n",
    "        for tag in list(row['tag'])[:3]\n",
    "    ]\n",
    "    while len(tag_vectors) < 3:  # Pad with zero vectors if fewer than 3 tags\n",
    "        tag_vectors.append(padding_vector)\n",
    "    \n",
    "    # Flatten the tag vectors (3 vectors of size 10 each -> 30 elements)\n",
    "    tag_vector = np.concatenate(tag_vectors)\n",
    "    \n",
    "    # Add genre one-hot encoding\n",
    "    genre_vector = row[unique_genres].values  # One-hot encoded genres\n",
    "    \n",
    "    # Combine tag vector and genre vector\n",
    "    feature_vector = np.concatenate([tag_vector, genre_vector])\n",
    "    return feature_vector\n",
    "filtered_data['feature_vector'] = filtered_data.apply(create_feature_vector, axis=1)\n",
    "\n",
    "#Okay so I mixed the tag vector and the genre vectors into one feature vector\n",
    "#I am going to drop the original tag vector and the genres\n",
    "filtered_data = filtered_data.drop(columns=list(unique_genres))\n",
    "filtered_data = filtered_data.drop(columns='tag')\n",
    "\n",
    "#wrote to file to see feature data\n",
    "# filtered_data.to_csv('filtered_movies.csv',index=False)\n",
    "\n",
    "#Block 4: The actual recommendation\n",
    "\n",
    "#TODO: implement the test and training sets split\n",
    "\n",
    "#Query is the movie we want to isolate\n",
    "#Cosine Similarities\n",
    "def knn_recommendation_cos(query,train_data,k=5):\n",
    "    #Extract ratings and feature vectors from training set\n",
    "    #Ratings are optional if you don't want them, I have them here to use as bias\n",
    "    train_features = np.array(train_data['feature_vector'].tolist())\n",
    "    train_ratings = train_data['average_rating'].values#Optional\n",
    "\n",
    "    #kNN using cosine_similarity\n",
    "    similarities = cosine_similarity([query],train_features).flatten()\n",
    "\n",
    "    #Optional Bias\n",
    "    weighted_similarities = similarities * train_ratings\n",
    "\n",
    "    #get the indicies of nearest movies (5)\n",
    "    kNN_indices = np.argsort(weighted_similarities)[-k:][::-1] #Sorted by weighted similariy in descending order\n",
    "\n",
    "    #Get the movies from the index\n",
    "    kNN_movies = train_data.iloc[kNN_indices]\n",
    "    return kNN_movies\n",
    "\n",
    "#if you want to compare cosine to euclidean distance\n",
    "def knn_recommendation_eucl(query,train_data,k=5):\n",
    "    #Extract ratings and feature vectors from training set\n",
    "    #Ratings are optional if you don't want them, I have them here to use as bias\n",
    "    train_features = np.array(train_data['feature_vector'].tolist())\n",
    "    train_ratings = train_data['average_rating'].values#Optional\n",
    "\n",
    "    #kNN using euclidean distances\n",
    "    distances = euclidean_distances([query],train_features).flatten()\n",
    "\n",
    "    #Optional Bias\n",
    "    weighted_distances= distances / train_ratings\n",
    "\n",
    "    #get the indicies of nearest movies (5)\n",
    "    kNN_indices = np.argsort(weighted_distances)[:k] #Sorted by weighted distances ascending order\n",
    "\n",
    "    #Get the movies from the index\n",
    "    kNN_movies = train_data.iloc[kNN_indices]\n",
    "    return kNN_movies\n",
    "\n",
    "\n",
    "def test_knn(test_data,train_data,k=5):\n",
    "    for _,row in test_data.iterrows():\n",
    "        query = np.array(row['feature_vector'])\n",
    "        knn_movies = knn_recommendation_cos(query,train_data,k)\n",
    "        print(f\"Current Movie searched: {row['title']}\")\n",
    "        print(\"Recommended Movies: \")\n",
    "        print(knn_movies[['title','average_rating']])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the combination of filter_data and rating\n",
    "# The dataframe will have the users grouped with the movies feature_vector\n",
    "def my_train(df, filt_data):\n",
    "    # Create empty lists to store train and test data\n",
    "    train_list = []\n",
    "\n",
    "    merged_df = pd.merge(df, filt_data, on='movieId')\n",
    "    grouped = merged_df.groupby('userId')\n",
    "\n",
    "    for user, group in grouped:\n",
    "        # Sort movies by rating in descending order\n",
    "        sorted_group = group.sort_values(by='movieId', ascending=True)\n",
    "        \n",
    "        # Use the rest for training\n",
    "        train = sorted_group.iloc[:]\n",
    "        \n",
    "        # Append to respective lists\n",
    "        train_list.append(train)\n",
    "\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Movie searched: Toy Story (1995)\n",
      "Recommended Movies: \n",
      "                                           title  average_rating\n",
      "0                               Toy Story (1995)             3.9\n",
      "664                           Toy Story 2 (1999)             3.9\n",
      "1010                         Finding Nemo (2003)             4.0\n",
      "807                                 Shrek (2001)             3.9\n",
      "1479                       The Lego Movie (2014)             3.9\n",
      "1268  Cat Returns, The (Neko no ongaeshi) (2002)             3.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = my_train(rate, filtered_data)\n",
    "# specific user and movie\n",
    "user_id = 1\n",
    "user = train_data[train_data['userId'] == user_id]\n",
    "movie = user[user['movieId'] == 1] # movie has to be valid movie of user\n",
    "\n",
    "# print(movie['feature_vector'])\n",
    "test_knn(movie, filtered_data, 6)\n",
    "\n",
    "# This will print recommendations for all user's movies\n",
    "# test_knn(user, filtered_data, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies for Jumanji (1995):\n",
      "861     Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1385     Chronicles of Narnia: Prince Caspian, The (2008)\n",
      "1                                          Jumanji (1995)\n",
      "1335    Chronicles of Narnia: The Lion, the Witch and ...\n",
      "201                              Wizard of Oz, The (1939)\n",
      "1268           Cat Returns, The (Neko no ongaeshi) (2002)\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# test specifict movie (*some movies ids do not exit, Reason: not enough data)\n",
    "movie_id = 2\n",
    "movie_by_id = filtered_data[filtered_data['movieId'] == movie_id]\n",
    "movie_querry = np.array(movie_by_id['feature_vector'].iloc[0])\n",
    "\n",
    "# test specifict movie\n",
    "knn_movies = knn_recommendation_cos(movie_querry, filtered_data, 6)\n",
    "print(f\"Recommended movies for {movie_by_id['title'].values[0]}:\\n{knn_movies['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for User 4 : {'Accuracy': 0.88, 'Precision': 0.88, 'Recall': 1.0, 'Confusion Matrix': array([[ 0,  3],\n",
      "       [ 0, 22]])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def user_based_knn_evaluation(user_id, ratings, filtered_data, k=5):\n",
    "    # Step 1: Extract Data for the User\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    user_data = pd.merge(user_ratings, filtered_data, on='movieId')\n",
    "    user_data['target'] = (user_data['rating'] >= 3).astype(int)  # Good = 1, Bad = 0\n",
    "    \n",
    "    # Step 2: Split Data into Training and Testing\n",
    "    train_data, test_data = train_test_split(user_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 3: Train and Test the kNN Model\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        query_vector = np.array(row['feature_vector'])\n",
    "        k_nearest_movies = knn_recommendation_cos(query_vector, train_data, k=k)\n",
    "        \n",
    "        # Majority vote for the target (Good/Bad) among k neighbors\n",
    "        majority_vote = k_nearest_movies['target'].mode()[0]\n",
    "        y_true.append(row['target'])\n",
    "        y_pred.append(majority_vote)\n",
    "    \n",
    "    # Step 4: Compute Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Confusion Matrix\": conf_matrix,\n",
    "    }\n",
    "\n",
    "# Example Usage\n",
    "user_id = 4  # Specify the user ID\n",
    "metrics = user_based_knn_evaluation(user_id, rate, filtered_data, k=5)\n",
    "print(\"Metrics for User\", user_id, \":\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
