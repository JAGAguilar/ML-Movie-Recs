{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block One: First Filtering\n",
    "#reading the CSV files, change them on your personal computer to wherever you saved them\n",
    "rate = pd.read_csv(r'ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv(r'ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv(r'ml-latest-small/ratings.csv')\n",
    "tags = pd.read_csv(r'ml-latest-small/tags.csv')\n",
    "userID = ratings\n",
    "#dropping timestamp as it's unnecessary\n",
    "tags = tags.drop(columns='timestamp')\n",
    "ratings = ratings.drop(columns='timestamp') \n",
    "\n",
    "#Groups on userId and movieId and groups tags into a list\n",
    "tags = tags.groupby(['movieId']).agg({'tag':set}).reset_index()\n",
    "\n",
    "#Refining ratings to just be movieId and the average rating to only 1 decimal point\n",
    "ratings = ratings.groupby('movieId')['rating'].mean().reset_index()\n",
    "ratings = ratings.rename(columns={'rating':'average_rating'})\n",
    "ratings['average_rating'] = ratings['average_rating'].round(1)\n",
    "filtered_data = pd.merge(movies,ratings, on='movieId')\n",
    "filtered_data = pd.merge(filtered_data, tags, on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block Two: Unique Genres and Second Filtering\n",
    "# Split genres by '|' and get all unique genres\n",
    "unique_genres = set()\n",
    "for genres in movies['genres']:\n",
    "    unique_genres.update(genres.split('|'))\n",
    "unique_genres = sorted(unique_genres)  # Sort in alphabetic order\n",
    "for genre in unique_genres:\n",
    "    filtered_data[genre] = filtered_data['genres'].apply(lambda x: 1 if genre in x.split('|') else 0)\n",
    "#filter out In Netflix Queue\n",
    "filtered_data['tag'] = filtered_data['tag'].apply(lambda tag_set: {tag for tag in tag_set if tag != \"In Netflix queue\"})\n",
    "\n",
    "# Drop the original 'genres' column (I think it's a good idea, no need to clutter the data yk?)\n",
    "filtered_data= filtered_data.drop(columns=['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block Three: Vectorization\n",
    "#I changed the vector size to 10 just cause it's smaller but if you wanna increase it by all means go for it\n",
    "wordVector = Word2Vec(filtered_data['tag'].tolist(),vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "vector_size = 10  # Same size as Word2Vec vectors\n",
    "padding_vector = np.zeros(vector_size)\n",
    "# Function to generate a vector for up to 3 tags\n",
    "def create_feature_vector(row):\n",
    "    # Get word vectors for the tags, up to 3 tags, and pad if fewer\n",
    "    tag_vectors = [\n",
    "        wordVector.wv[tag] if tag in wordVector.wv else padding_vector\n",
    "        for tag in list(row['tag'])[:3]\n",
    "    ]\n",
    "    while len(tag_vectors) < 3:  # Pad with zero vectors if fewer than 3 tags\n",
    "        tag_vectors.append(padding_vector)\n",
    "    \n",
    "    # Flatten the tag vectors (3 vectors of size 10 each -> 30 elements)\n",
    "    tag_vector = np.concatenate(tag_vectors)\n",
    "    \n",
    "    # Add genre one-hot encoding\n",
    "    genre_vector = row[unique_genres].values  # One-hot encoded genres\n",
    "    \n",
    "    # Combine tag vector and genre vector\n",
    "    feature_vector = np.concatenate([tag_vector, genre_vector])\n",
    "    return feature_vector\n",
    "filtered_data['feature_vector'] = filtered_data.apply(create_feature_vector, axis=1)\n",
    "\n",
    "#Okay so I mixed the tag vector and the genre vectors into one feature vector\n",
    "#I am going to drop the original tag vector and the genres\n",
    "filtered_data = filtered_data.drop(columns=list(unique_genres))\n",
    "filtered_data = filtered_data.drop(columns='tag')\n",
    "\n",
    "#wrote to file to see feature data\n",
    "# filtered_data.to_csv('filtered_movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 4: The actual recommendation\n",
    "\n",
    "#TODO: implement the test and training sets split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query is the movie we want to isolate\n",
    "#Cosine Similarities\n",
    "def knn_recommendation_cos(query,train_data, user_watched_movies, k=5):\n",
    "    #Extract ratings and feature vectors from training set\n",
    "    #Ratings are optional if you don't want them, I have them here to use as bias\n",
    "    train_features = np.array(train_data['feature_vector'].tolist())\n",
    "    train_ratings = train_data['average_rating'].values#Optional\n",
    "\n",
    "    #kNN using cosine_similarity\n",
    "    similarities = cosine_similarity([query],train_features).flatten()\n",
    "\n",
    "    #Optional Bias\n",
    "    weighted_similarities = similarities * train_ratings\n",
    "\n",
    "    # #get the indicies of nearest movies (5)\n",
    "    # kNN_indices = np.argsort(weighted_similarities)[-k:][::-1] #Sorted by weighted similariy in descending order\n",
    "\n",
    "    # #Get the movies from the index\n",
    "    # kNN_movies = train_data.iloc[kNN_indices]\n",
    "        # Get the indices of nearest movies (more than k initially)\n",
    "    kNN_indices = np.argsort(weighted_similarities)[-k*2:][::-1]  # Get twice the number of recommendations (more than needed)\n",
    "\n",
    "    # Get the movies from the index\n",
    "    kNN_movies = train_data.iloc[kNN_indices]\n",
    "\n",
    "    # Remove the movies the user has already watched\n",
    "    kNN_movies_filtered = kNN_movies[~kNN_movies['movieId'].isin(user_watched_movies)]\n",
    "\n",
    "    # If we have fewer than k movies after filtering, fetch more from the remaining pool\n",
    "    if len(kNN_movies_filtered) < k:\n",
    "        # Find additional movies (not already recommended) by getting the remaining top recommendations\n",
    "        remaining_movies = kNN_movies[~kNN_movies['movieId'].isin(kNN_movies_filtered['movieId'])]\n",
    "        additional_movies_needed = k - len(kNN_movies_filtered)\n",
    "        additional_movies = remaining_movies.head(additional_movies_needed)\n",
    "        \n",
    "        # Combine the filtered list and additional recommendations\n",
    "        kNN_movies_filtered = pd.concat([kNN_movies_filtered, additional_movies])\n",
    "\n",
    "    return kNN_movies_filtered.head(k)  # Ensure we return exactly k recommendations\n",
    "    # return kNN_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to compare cosine to euclidean distance\n",
    "def knn_recommendation_eucl(query,train_data,k=5):\n",
    "    #Extract ratings and feature vectors from training set\n",
    "    #Ratings are optional if you don't want them, I have them here to use as bias\n",
    "    train_features = np.array(train_data['feature_vector'].tolist())\n",
    "    train_ratings = train_data['average_rating'].values#Optional\n",
    "\n",
    "    #kNN using euclidean distances\n",
    "    distances = euclidean_distances([query],train_features).flatten()\n",
    "\n",
    "    #Optional Bias\n",
    "    weighted_distances= distances / train_ratings\n",
    "\n",
    "    #get the indicies of nearest movies (5)\n",
    "    kNN_indices = np.argsort(weighted_distances)[:k] #Sorted by weighted distances ascending order\n",
    "\n",
    "    #Get the movies from the index\n",
    "    kNN_movies = train_data.iloc[kNN_indices]\n",
    "    return kNN_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(test_data,train_data,user_watched_movies, k=5):\n",
    "    # Dictionary to store movie frequencies\n",
    "    movie_frequency = {}\n",
    "\n",
    "    for _,row in test_data.iterrows():\n",
    "        query = np.array(row['feature_vector'])\n",
    "        knn_movies = knn_recommendation_cos(query,train_data,user_watched_movies,k)\n",
    "        # print(f\"Current Movie searched: {row['title']}\")\n",
    "        # print(\"Recommended Movies: \")\n",
    "        # print(knn_movies[['title','average_rating']])\n",
    "        # print()\n",
    "\n",
    "        # Update the frequency dictionary\n",
    "        for _, movie_row in knn_movies.iterrows():\n",
    "            movie_title = movie_row['title']\n",
    "            movie_id = movie_row['movieId']\n",
    "            if movie_title in movie_frequency:\n",
    "                movie_frequency[movie_title][0] += 1\n",
    "            else:\n",
    "                movie_frequency[movie_title] = [1, movie_id]\n",
    "\n",
    "    # Sort the movie_frequency dictionary by frequency (the first element of the list)\n",
    "    sorted_dict = dict(sorted(movie_frequency.items(), key=lambda item: item[1][0], reverse=True))\n",
    "\n",
    "    # Get the movie with the maximum frequency\n",
    "    max_key = max(movie_frequency, key=lambda k: movie_frequency[k][0])  # Get the movie with the highest frequency\n",
    "    max_value = movie_frequency[max_key]  # The value is a list: [frequency, movie_id]\n",
    "\n",
    "    print(f\"Maximum value: {max_value[0]} with Key: {max_key} and Movie ID: {max_value[1]}\")\n",
    "\n",
    "    # Print the top 5 recommended movies\n",
    "    count = 0\n",
    "    movieIds = []\n",
    "    for movie, freq in sorted_dict.items():\n",
    "        print(f\"{movie}: Frequency = {freq[0]}, Movie ID = {freq[1]}\")  # Display frequency and Movie ID\n",
    "        movieIds.append(freq[1])\n",
    "        if count == 5:\n",
    "            break\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    return movieIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(movie_id_1, movie_id_2, train_data):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two movies based on their feature vectors.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the feature vectors for the two movies\n",
    "    movie_1_vector = train_data.loc[train_data['movieId'] == movie_id_1, 'feature_vector'].values[0]\n",
    "    movie_2_vector = train_data.loc[train_data['movieId'] == movie_id_2, 'feature_vector'].values[0]\n",
    "\n",
    "    # Reshape the vectors to 2D arrays (required for cosine_similarity)\n",
    "    movie_1_vector = movie_1_vector.reshape(1, -1)\n",
    "    movie_2_vector = movie_2_vector.reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(movie_1_vector, movie_2_vector)[0][0]\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(df, n=7):\n",
    "    # Create empty lists to store train and test data\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # Group by 'userID' to process each user individually\n",
    "    \n",
    "    merged_df = pd.merge(df, filtered_data, on='movieId')\n",
    "    grouped = merged_df.groupby('userId')\n",
    "\n",
    "    for user, group in grouped:\n",
    "        # Sort movies by rating in descending order\n",
    "        sorted_group = group.sort_values(by='rating', ascending=False)\n",
    "        \n",
    "        # Select the top 3 rated movies for testing\n",
    "        test = sorted_group.head(n)\n",
    "        \n",
    "        # Use the rest for training\n",
    "        train = sorted_group.iloc[n:]\n",
    "        \n",
    "        # Append to respective lists\n",
    "        test_list.append(test)\n",
    "        train_list.append(train)\n",
    "\n",
    "    # Combine all train and test splits into DataFrames\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "    print(\"Training Data:\")\n",
    "    print(train_data)\n",
    "    print(\"\\nTesting Data:\")\n",
    "    print(test_data)\n",
    "    print(type(train_data))\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "       userId  movieId  rating   timestamp  \\\n",
      "0           1     1240     5.0   964983723   \n",
      "1           1     2139     5.0   964982791   \n",
      "2           1     2115     5.0   964982529   \n",
      "3           1     2078     5.0   964982838   \n",
      "4           1     2058     5.0   964982400   \n",
      "...       ...      ...     ...         ...   \n",
      "48282     610    96861     2.0  1493850474   \n",
      "48283     610    69526     2.0  1493846153   \n",
      "48284     610     6541     1.5  1493845480   \n",
      "48285     610   120635     1.0  1493850489   \n",
      "48286     610    68319     1.0  1493845505   \n",
      "\n",
      "                                                   title  average_rating  \\\n",
      "0                                 Terminator, The (1984)             3.9   \n",
      "1                             Secret of NIMH, The (1982)             3.5   \n",
      "2            Indiana Jones and the Temple of Doom (1984)             3.6   \n",
      "3                                Jungle Book, The (1967)             3.8   \n",
      "4                                 Negotiator, The (1998)             3.4   \n",
      "...                                                  ...             ...   \n",
      "48282                                     Taken 2 (2012)             3.3   \n",
      "48283         Transformers: Revenge of the Fallen (2009)             2.4   \n",
      "48284  League of Extraordinary Gentlemen, The (a.k.a....             2.6   \n",
      "48285                                     Taken 3 (2015)             2.7   \n",
      "48286                    X-Men Origins: Wolverine (2009)             2.9   \n",
      "\n",
      "                                          feature_vector  \n",
      "0      [0.09726841002702713, -0.07475874572992325, -0...  \n",
      "1      [0.07489033043384552, -0.08546503633260727, 0....  \n",
      "2      [-0.034710951149463654, 0.017216186970472336, ...  \n",
      "3      [0.07349581271409988, 0.05029662325978279, 0.0...  \n",
      "4      [-0.05047808215022087, -0.06758777797222137, -...  \n",
      "...                                                  ...  \n",
      "48282  [-0.024538569152355194, -0.05947592481970787, ...  \n",
      "48283  [0.09924023598432541, 0.08376741409301758, -0....  \n",
      "48284  [0.07344192266464233, 0.07700993865728378, 0.0...  \n",
      "48285  [0.030864253640174866, -0.005395594518631697, ...  \n",
      "48286  [0.08694885671138763, 0.0674978569149971, 0.03...  \n",
      "\n",
      "[48287 rows x 7 columns]\n",
      "\n",
      "Testing Data:\n",
      "Empty DataFrame\n",
      "Columns: [userId, movieId, rating, timestamp, title, average_rating, feature_vector]\n",
      "Index: []\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = my_train_test_split(rate, n=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [userId, movieId, rating, timestamp, title, average_rating, feature_vector]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe for that specific user\n",
    "# print(type(train_data))\n",
    "# print(train_data)\n",
    "user_data = train_data[train_data['userId'] == 11]\n",
    "user_data1 = test_data[test_data['userId'] == 11]\n",
    "# print(user_data)\n",
    "print(user_data1)\n",
    "\n",
    "# test_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 5 with Key: Matrix, The (1999) and Movie ID: 2571\n",
      "Matrix, The (1999): Frequency = 5, Movie ID = 2571\n",
      "Blade Runner (1982): Frequency = 4, Movie ID = 541\n",
      "Star Trek: First Contact (1996): Frequency = 4, Movie ID = 1356\n",
      "North by Northwest (1959): Frequency = 4, Movie ID = 908\n",
      "Woman Under the Influence, A (1974): Frequency = 3, Movie ID = 7071\n",
      "Two Family House (2000): Frequency = 3, Movie ID = 3951\n"
     ]
    }
   ],
   "source": [
    "user_watched_movies = user_data['movieId'].to_numpy()\n",
    "# print(user_watched_movies)\n",
    "movieIDs = test_knn(user_data, filtered_data, user_watched_movies, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movie: 2571 Similarity:0.4040010396691586\n",
      "Recommended movie: 541 Similarity:0.4048810815297864\n",
      "Recommended movie: 1356 Similarity:0.41319433638300096\n",
      "Recommended movie: 908 Similarity:0.4180905888616713\n",
      "Recommended movie: 7071 Similarity:0.3902671511958666\n",
      "Recommended movie: 3951 Similarity:0.3717181927519968\n"
     ]
    }
   ],
   "source": [
    "user_test_movies = user_data['movieId'].to_numpy()\n",
    "\n",
    "arr = []\n",
    "\n",
    "for x in movieIDs:\n",
    "    for y in user_test_movies:\n",
    "        z = calculate_cosine_similarity(x, y, filtered_data)\n",
    "        arr.append(z)\n",
    "\n",
    "    print(f\"Recommended movie: {x} Similarity:{np.mean(arr)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
