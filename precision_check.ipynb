{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId                                      title  \\\n",
      "0           1                           Toy Story (1995)   \n",
      "1           2                             Jumanji (1995)   \n",
      "2           3                    Grumpier Old Men (1995)   \n",
      "3           4                   Waiting to Exhale (1995)   \n",
      "4           5         Father of the Bride Part II (1995)   \n",
      "...       ...                                        ...   \n",
      "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
      "9738   193583               No Game No Life: Zero (2017)   \n",
      "9739   193585                               Flint (2017)   \n",
      "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
      "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
      "\n",
      "                                           genres  average_rating  \\\n",
      "0     Adventure|Animation|Children|Comedy|Fantasy             3.9   \n",
      "1                      Adventure|Children|Fantasy             3.4   \n",
      "2                                  Comedy|Romance             3.3   \n",
      "3                            Comedy|Drama|Romance             2.4   \n",
      "4                                          Comedy             3.1   \n",
      "...                                           ...             ...   \n",
      "9737              Action|Animation|Comedy|Fantasy             4.0   \n",
      "9738                     Animation|Comedy|Fantasy             3.5   \n",
      "9739                                        Drama             3.5   \n",
      "9740                             Action|Animation             3.5   \n",
      "9741                                       Comedy             4.0   \n",
      "\n",
      "                                                    tag  \n",
      "0                                          {pixar, fun}  \n",
      "1     {magic board game, Robin Williams, fantasy, game}  \n",
      "2                                          {old, moldy}  \n",
      "3                                                    {}  \n",
      "4                                   {remake, pregnancy}  \n",
      "...                                                 ...  \n",
      "9737                                                 {}  \n",
      "9738                                                 {}  \n",
      "9739                                                 {}  \n",
      "9740                                                 {}  \n",
      "9741                                                 {}  \n",
      "\n",
      "[9742 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Block One: First Filtering\n",
    "#reading the CSV files, change them on your personal computer to wherever you saved them\n",
    "rate = pd.read_csv(r'ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv(r'ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv(r'ml-latest-small/ratings.csv')\n",
    "tags = pd.read_csv(r'ml-latest-small/tags.csv')\n",
    "userID = ratings\n",
    "#dropping timestamp as it's unnecessary\n",
    "tags = tags.drop(columns='timestamp')\n",
    "ratings = ratings.drop(columns='timestamp') \n",
    "\n",
    "#Groups on userId and movieId and groups tags into a list\n",
    "tags = tags.groupby(['movieId']).agg({'tag':set}).reset_index()\n",
    "\n",
    "#Refining ratings to just be movieId and the average rating to only 1 decimal point\n",
    "ratings = ratings.groupby('movieId')['rating'].mean().reset_index()\n",
    "ratings = ratings.rename(columns={'rating':'average_rating'})\n",
    "ratings['average_rating'] = ratings['average_rating'].round(1)\n",
    "# Merge movies with ratings (outer join to keep all movies)\n",
    "filtered_data = pd.merge(movies, ratings, on='movieId', how='left')\n",
    "\n",
    "# Merge the result with tags (outer join to keep all movies)\n",
    "filtered_data = pd.merge(filtered_data, tags, on='movieId', how='left')\n",
    "\n",
    "# Replace NaN in the 'average_rating' and 'tag' columns with default values\n",
    "filtered_data['average_rating'] = filtered_data['average_rating'].fillna(0)  # Default rating for movies with no ratings\n",
    "filtered_data['tag'] = filtered_data['tag'].apply(lambda x: x if isinstance(x, set) else set())  # Default empty set for movies with no tags\n",
    "\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Block Two: Unique Genres and Second Filtering\n",
    "# Split genres by '|' and get all unique genres\n",
    "unique_genres = set()\n",
    "for genres in movies['genres']:\n",
    "    unique_genres.update(genres.split('|'))\n",
    "unique_genres = sorted(unique_genres)  # Sort in alphabetic order\n",
    "print(len(unique_genres))\n",
    "for genre in unique_genres:\n",
    "    filtered_data[genre] = filtered_data['genres'].apply(lambda x: 1 if genre in x.split('|') else 0)\n",
    "#filter out In Netflix Queue\n",
    "filtered_data['tag'] = filtered_data['tag'].apply(lambda tag_set: {tag for tag in tag_set if tag != \"In Netflix queue\"})\n",
    "# print(filtered_data.head())\n",
    "\n",
    "# Drop the original 'genres' column (I think it's a good idea, no need to clutter the data yk?)\n",
    "filtered_data= filtered_data.drop(columns=['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block Three: Vectorization\n",
    "#I changed the vector size to 10 just cause it's smaller but if you wanna increase it by all means go for it\n",
    "wordVector = Word2Vec(filtered_data['tag'].tolist(),vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "vector_size = 10  # Same size as Word2Vec vectors\n",
    "padding_vector = np.zeros(vector_size)\n",
    "# Function to generate a vector for up to 3 tags\n",
    "def create_feature_vector(row):\n",
    "    # Get word vectors for the tags, up to 3 tags, and pad if fewer\n",
    "    tag_vectors = [\n",
    "        wordVector.wv[tag] if tag in wordVector.wv else padding_vector\n",
    "        for tag in list(row['tag'])[:3] # inlude if needed\n",
    "    ]\n",
    "    while len(tag_vectors) < 3:  # Pad with zero vectors if fewer than 3 tags\n",
    "        tag_vectors.append(padding_vector)\n",
    "    \n",
    "    # Flatten the tag vectors (3 vectors of size 10 each -> 30 elements)\n",
    "    tag_vector = np.concatenate(tag_vectors)\n",
    "    \n",
    "    # Add genre one-hot encoding\n",
    "    genre_vector = row[unique_genres].values  # One-hot encoded genres\n",
    "    \n",
    "    # Combine tag vector and genre vector\n",
    "    feature_vector = np.concatenate([tag_vector, genre_vector])\n",
    "    return feature_vector\n",
    "\n",
    "#wrote to file to see feature data\n",
    "# filtered_data.to_csv('filtered_movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \\\n",
      "0  Adventure|Animation|Children|Comedy|Fantasy   \n",
      "1                   Adventure|Children|Fantasy   \n",
      "2                               Comedy|Romance   \n",
      "3                         Comedy|Drama|Romance   \n",
      "4                                       Comedy   \n",
      "\n",
      "                                        genre_vector  \\\n",
      "0  [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                                 tag  \\\n",
      "0                                       {pixar, fun}   \n",
      "1  {magic board game, Robin Williams, fantasy, game}   \n",
      "2                                       {old, moldy}   \n",
      "3                                                 {}   \n",
      "4                                {remake, pregnancy}   \n",
      "\n",
      "                                            tag_list  \\\n",
      "0                                       [pixar, fun]   \n",
      "1  [magic board game, robin williams, fantasy, game]   \n",
      "2                                       [old, moldy]   \n",
      "3                                                 []   \n",
      "4                                [remake, pregnancy]   \n",
      "\n",
      "                                          tag_vector  \n",
      "0  [0.0023323027, 0.009218204, -0.0082672145, 0.0...  \n",
      "1  [-0.0004415987, -0.011798444, -0.004707656, -0...  \n",
      "2  [-0.025493741, -0.016653417, 0.0038747345, -0....  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.01571066, -0.016242635, 0.0028820056, -0.00...  \n"
     ]
    }
   ],
   "source": [
    "movies_df = pd.read_csv('ml-latest-small/movies.csv')\n",
    "tags_df = pd.read_csv('ml-latest-small/tags.csv')\n",
    "tags_set = tags_df.groupby(['movieId']).agg({'tag':set}).reset_index()\n",
    "\n",
    "\"\"\"      Genres_vector     \"\"\"\n",
    "# Initialize the genre_vector list\n",
    "genre_vector = []\n",
    "\n",
    "# Iterate over each movie's genres\n",
    "for _, row in movies_df.iterrows():\n",
    "    # Create a one-hot vector for the current movie\n",
    "    genres = row['genres'].split('|')\n",
    "    vector = [1 if genre in genres else 0 for genre in unique_genres]\n",
    "    genre_vector.append(vector)\n",
    "\n",
    "# Convert to a numpy array for further use\n",
    "genre_vector = np.array(genre_vector)\n",
    "\n",
    "# Add the genre_vector to the dataframe for reference (optional)\n",
    "movies_df['genre_vector'] = list(genre_vector)\n",
    "# print(f\"\\n ------This is genre-vec len: {len(movies_df['genre_vector'][0])} --------- \\n\")\n",
    "\n",
    "\n",
    "\"\"\"       Tags_vector      \"\"\"\n",
    "# 3. Merge tags with movies, ensuring all movies are included\n",
    "movies_df = movies_df.merge(tags_set, how='left', left_on='movieId', right_on='movieId')\n",
    "\n",
    "# 4. Replace NaN tags with an empty set\n",
    "movies_df['tag'] = movies_df['tag'].apply(lambda x: x if isinstance(x, set) else set())\n",
    "\n",
    "# 5. Preprocess tags: Split and clean\n",
    "movies_df['tag_list'] = movies_df['tag'].apply(lambda x: [tag.strip().lower() for tag in x])\n",
    "\n",
    "# 3. Train word2vec model\n",
    "# Combine all tag lists into one list of lists\n",
    "tag_corpus = movies_df['tag_list'].tolist()\n",
    "\n",
    "# Train a word2vec model\n",
    "word2vec_model = Word2Vec(tag_corpus, vector_size=30, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 4. Create movie vectors by averaging tag vectors\n",
    "def get_movie_vector(tags):\n",
    "    tag_vectors = [word2vec_model.wv[tag] for tag in tags if tag in word2vec_model.wv]\n",
    "    return np.mean(tag_vectors, axis=0) if tag_vectors else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "movies_df['tag_vector'] = movies_df['tag_list'].apply(get_movie_vector)\n",
    "# print(f\"\\n ------This is movie-vec len: {len(movies_df['tag_vector'][0])} --------- \\n\")\n",
    "\n",
    "# 5. Drop unnecessary columns if needed\n",
    "# movies_df = movies_df.drop(columns=['tags', 'tag_list'])\n",
    "\n",
    "print(movies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "   movieId                               title  average_rating  \\\n",
      "0        1                    Toy Story (1995)             3.9   \n",
      "1        2                      Jumanji (1995)             3.4   \n",
      "2        3             Grumpier Old Men (1995)             3.3   \n",
      "3        4            Waiting to Exhale (1995)             2.4   \n",
      "4        5  Father of the Bride Part II (1995)             3.1   \n",
      "\n",
      "                                      feature_vector  \n",
      "0  [0.0023323027417063713, 0.009218203835189342, ...  \n",
      "1  [-0.00044159870594739914, -0.01179844420403242...  \n",
      "2  [-0.025493741035461426, -0.01665341667830944, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.015710659325122833, -0.016242634505033493, ...  \n"
     ]
    }
   ],
   "source": [
    "filtered_data_copy = filtered_data.copy()\n",
    "\n",
    "# What we used before\n",
    "filtered_data['feature_vector'] = filtered_data.apply(create_feature_vector, axis=1)\n",
    "\n",
    "# What we are using now\n",
    "movies_df['feature_vector'] = movies_df.apply(\n",
    "    lambda row: np.concatenate([row['tag_vector'], row['genre_vector']]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "filtered_data1 = filtered_data_copy.merge(\n",
    "    movies_df[['movieId', 'feature_vector']],  # Select only necessary columns\n",
    "    on='movieId',  # Merge on the 'movieId' column\n",
    "    how='left'  # Use a left join to keep all rows in filtered_data_df\n",
    ")\n",
    "\n",
    "#Okay so I mixed the tag vector and the genre vectors into one feature vector\n",
    "#I am going to drop the original tag vector and the genres\n",
    "filtered_data = filtered_data.drop(columns=list(unique_genres))\n",
    "filtered_data = filtered_data.drop(columns='tag')\n",
    "\n",
    "filtered_data1 = filtered_data1.drop(columns=list(unique_genres))\n",
    "filtered_data1 = filtered_data1.drop(columns='tag')\n",
    "\n",
    "print(len(filtered_data1['feature_vector'][6]))\n",
    "print(filtered_data1.head())\n",
    "# filtered_data1.to_csv('filtered_movies.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query is the movie we want to isolate\n",
    "#Cosine Similarities\n",
    "def knn_recommendation_cos(query,train_data, user_watched_movies, k=5):\n",
    "    #Extract ratings and feature vectors from training set\n",
    "    #Ratings are optional if you don't want them, I have them here to use as bias\n",
    "    train_features = np.array(train_data['feature_vector'].tolist())\n",
    "    train_ratings = train_data['average_rating'].values#Optional\n",
    "\n",
    "    #kNN using cosine_similarity\n",
    "    similarities = cosine_similarity([query],train_features).flatten()\n",
    "\n",
    "    #Optional Bias\n",
    "    weighted_similarities = similarities * train_ratings\n",
    "\n",
    "    # #get the indicies of nearest movies (5)\n",
    "    # kNN_indices = np.argsort(weighted_similarities)[-k:][::-1] #Sorted by weighted similariy in descending order\n",
    "\n",
    "    # #Get the movies from the index\n",
    "    # kNN_movies = train_data.iloc[kNN_indices]\n",
    "        # Get the indices of nearest movies (more than k initially)\n",
    "    kNN_indices = np.argsort(weighted_similarities)[-k*2:][::-1]  # Get twice the number of recommendations (more than needed)\n",
    "\n",
    "    # Get the movies from the index\n",
    "    kNN_movies = train_data.iloc[kNN_indices]\n",
    "\n",
    "    # Remove the movies the user has already watched\n",
    "    kNN_movies_filtered = kNN_movies[~kNN_movies['movieId'].isin(user_watched_movies)]\n",
    "\n",
    "    # If we have fewer than k movies after filtering, fetch more from the remaining pool\n",
    "    if len(kNN_movies_filtered) < k:\n",
    "        # Find additional movies (not already recommended) by getting the remaining top recommendations\n",
    "        remaining_movies = kNN_movies[~kNN_movies['movieId'].isin(kNN_movies_filtered['movieId'])]\n",
    "        additional_movies_needed = k - len(kNN_movies_filtered)\n",
    "        additional_movies = remaining_movies.head(additional_movies_needed)\n",
    "        \n",
    "        # Combine the filtered list and additional recommendations\n",
    "        kNN_movies_filtered = pd.concat([kNN_movies_filtered, additional_movies])\n",
    "\n",
    "    return kNN_movies_filtered.head(k)  # Ensure we return exactly k recommendations\n",
    "    # return kNN_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to compare cosine to euclidean distance\n",
    "def knn_recommendation_eucl(query,train_data,user_watched_movies,k=5):\n",
    "    #Extract ratings and feature vectors from training set\n",
    "    #Ratings are optional if you don't want them, I have them here to use as bias\n",
    "    train_features = np.array(train_data['feature_vector'].tolist())\n",
    "    train_ratings = train_data['average_rating'].values#Optional\n",
    "\n",
    "    #kNN using euclidean distances\n",
    "    distances = euclidean_distances([query],train_features).flatten()\n",
    "\n",
    "    # Optional bias (avoid division by zero)\n",
    "    epsilon = 1e-10\n",
    "    weighted_distances = distances / (train_ratings + epsilon)\n",
    "\n",
    "    #get the indicies of nearest movies (5)\n",
    "    # kNN_indices = np.argsort(weighted_distances)[:k] #Sorted by weighted distances ascending order\n",
    "    kNN_indices = np.argsort(weighted_distances)[-k*2:][::-1]  # Get twice the number of recommendations (more than needed)\n",
    "\n",
    "    # Get the movies from the index\n",
    "    kNN_movies = train_data.iloc[kNN_indices]\n",
    "\n",
    "    # Remove the movies the user has already watched\n",
    "    kNN_movies_filtered = kNN_movies[~kNN_movies['movieId'].isin(user_watched_movies)]\n",
    "\n",
    "    # If we have fewer than k movies after filtering, fetch more from the remaining pool\n",
    "    if len(kNN_movies_filtered) < k:\n",
    "        # Find additional movies (not already recommended) by getting the remaining top recommendations\n",
    "        remaining_movies = kNN_movies[~kNN_movies['movieId'].isin(kNN_movies_filtered['movieId'])]\n",
    "        additional_movies_needed = k - len(kNN_movies_filtered)\n",
    "        additional_movies = remaining_movies.head(additional_movies_needed)\n",
    "        \n",
    "        # Combine the filtered list and additional recommendations\n",
    "        kNN_movies_filtered = pd.concat([kNN_movies_filtered, additional_movies])\n",
    "\n",
    "    return kNN_movies_filtered.head(k)  # Ensure we return exactly k recommendations\n",
    "    #Get the movies from the index\n",
    "    # kNN_movies = train_data.iloc[kNN_indices]\n",
    "    # return kNN_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(test_data,train_data,user_watched_movies, k=5):\n",
    "    # Dictionary to store movie frequencies\n",
    "    movie_frequency = {}\n",
    "\n",
    "    for _,row in test_data.iterrows():\n",
    "        query = np.array(row['feature_vector'])\n",
    "        knn_movies = knn_recommendation_cos(query,train_data,user_watched_movies,k)\n",
    "        # print(f\"Current Movie searched: {row['title']}\")\n",
    "        # print(\"Recommended Movies: \")\n",
    "        # print(knn_movies[['title','average_rating']])\n",
    "        # print()\n",
    "\n",
    "        # Update the frequency dictionary\n",
    "        for _, movie_row in knn_movies.iterrows():\n",
    "            movie_title = movie_row['title']\n",
    "            movie_id = movie_row['movieId']\n",
    "            if movie_title in movie_frequency:\n",
    "                movie_frequency[movie_title][0] += 1\n",
    "            else:\n",
    "                movie_frequency[movie_title] = [1, movie_id]\n",
    "\n",
    "    # Sort the movie_frequency dictionary by frequency (the first element of the list)\n",
    "    sorted_dict = dict(sorted(movie_frequency.items(), key=lambda item: item[1][0], reverse=True))\n",
    "\n",
    "    # Get the movie with the maximum frequency\n",
    "    max_key = max(movie_frequency, key=lambda k: movie_frequency[k][0])  # Get the movie with the highest frequency\n",
    "    max_value = movie_frequency[max_key]  # The value is a list: [frequency, movie_id]\n",
    "\n",
    "    print(f\"Maximum value: {max_value[0]} with Key: {max_key} and Movie ID: {max_value[1]}\")\n",
    "\n",
    "    # Print the top 5 recommended movies\n",
    "    count = 0\n",
    "    movieIds = []\n",
    "    for movie, freq in sorted_dict.items():\n",
    "        print(f\"{movie}: Frequency = {freq[0]}, Movie ID = {freq[1]}\")  # Display frequency and Movie ID\n",
    "        movieIds.append(freq[1])\n",
    "        if count == 5:\n",
    "            break\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    return movieIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn_eucl(test_data,train_data,user_watched_movies, k=5):\n",
    "    # Dictionary to store movie frequencies\n",
    "    movie_frequency = {}\n",
    "\n",
    "    for _,row in test_data.iterrows():\n",
    "        query = np.array(row['feature_vector'])\n",
    "        knn_movies = knn_recommendation_eucl(query,train_data,user_watched_movies,k)\n",
    "        # print(f\"Current Movie searched: {row['title']}\")\n",
    "        # print(\"Recommended Movies: \")\n",
    "        # print(knn_movies[['title','average_rating']])\n",
    "        # print()\n",
    "\n",
    "        # Update the frequency dictionary\n",
    "        for _, movie_row in knn_movies.iterrows():\n",
    "            movie_title = movie_row['title']\n",
    "            movie_id = movie_row['movieId']\n",
    "            if movie_title in movie_frequency:\n",
    "                movie_frequency[movie_title][0] += 1\n",
    "            else:\n",
    "                movie_frequency[movie_title] = [1, movie_id]\n",
    "\n",
    "    # Sort the movie_frequency dictionary by frequency (the first element of the list)\n",
    "    sorted_dict = dict(sorted(movie_frequency.items(), key=lambda item: item[1][0], reverse=True))\n",
    "\n",
    "    # Get the movie with the maximum frequency\n",
    "    max_key = max(movie_frequency, key=lambda k: movie_frequency[k][0])  # Get the movie with the highest frequency\n",
    "    max_value = movie_frequency[max_key]  # The value is a list: [frequency, movie_id]\n",
    "\n",
    "    print(f\"Maximum value: {max_value[0]} with Key: {max_key} and Movie ID: {max_value[1]}\")\n",
    "\n",
    "    # Print the top 5 recommended movies\n",
    "    count = 0\n",
    "    movieIds = []\n",
    "    for movie, freq in sorted_dict.items():\n",
    "        print(f\"{movie}: Frequency = {freq[0]}, Movie ID = {freq[1]}\")  # Display frequency and Movie ID\n",
    "        movieIds.append(freq[1])\n",
    "        if count == 5:\n",
    "            break\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    return movieIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(movie_id_1, movie_id_2, train_data):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two movies based on their feature vectors.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the feature vectors for the two movies\n",
    "    movie_1_vector = train_data.loc[train_data['movieId'] == movie_id_1, 'feature_vector'].values[0]\n",
    "    # print(movie_1_vector)\n",
    "    movie_2_vector = train_data.loc[train_data['movieId'] == movie_id_2, 'feature_vector'].values[0]\n",
    "\n",
    "    # Reshape the vectors to 2D arrays (required for cosine_similarity)\n",
    "    movie_1_vector = movie_1_vector.reshape(1, -1)\n",
    "    movie_2_vector = movie_2_vector.reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(movie_1_vector, movie_2_vector)[0][0]\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def calculate_euclidean_distance(movie_id_1, movie_id_2, train_data):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two movies based on their feature vectors.\n",
    "\n",
    "    Args:\n",
    "        movie_id_1 (int): The ID of the first movie.\n",
    "        movie_id_2 (int): The ID of the second movie.\n",
    "        train_data (DataFrame): The DataFrame containing movie data with a 'feature_vector' column.\n",
    "\n",
    "    Returns:\n",
    "        float: The Euclidean distance between the two movies' feature vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the feature vectors for the two movies\n",
    "    movie_1_vector = train_data.loc[train_data['movieId'] == movie_id_1, 'feature_vector'].values[0]\n",
    "    movie_2_vector = train_data.loc[train_data['movieId'] == movie_id_2, 'feature_vector'].values[0]\n",
    "\n",
    "    # Calculate Euclidean distance\n",
    "    distance = euclidean(movie_1_vector, movie_2_vector)\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(df, filt_data, n=7):\n",
    "    # Create empty lists to store train and test data\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # Group by 'userID' to process each user individually\n",
    "    \n",
    "    merged_df = pd.merge(df, filt_data, on='movieId')\n",
    "    grouped = merged_df.groupby('userId')\n",
    "\n",
    "    for user, group in grouped:\n",
    "        # Sort movies by rating in descending order\n",
    "        sorted_group = group.sort_values(by='rating', ascending=False)\n",
    "        \n",
    "        # Select the top n rated movies for testing\n",
    "        test = sorted_group.head(n)\n",
    "        \n",
    "        # Use the rest for training\n",
    "        train = sorted_group.iloc[n:]\n",
    "        \n",
    "        # Append to respective lists\n",
    "        test_list.append(test)\n",
    "        train_list.append(train)\n",
    "\n",
    "    # Combine all train and test splits into DataFrames\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "    print(\"Training Data:\")\n",
    "    print(train_data)\n",
    "    print(\"\\nTesting Data:\")\n",
    "    print(test_data)\n",
    "    print(type(train_data))\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(df, filt_data):\n",
    "    # Create empty lists to store train and test data\n",
    "    train_list = []\n",
    "\n",
    "    merged_df = pd.merge(df, filt_data, on='movieId')\n",
    "    grouped = merged_df.groupby('userId')\n",
    "\n",
    "    for user, group in grouped:\n",
    "        # Sort movies by rating in descending order\n",
    "        sorted_group = group.sort_values(by='rating', ascending=False)\n",
    "        \n",
    "        # Use the rest for training\n",
    "        train = sorted_group.iloc[:]\n",
    "        \n",
    "        # Append to respective lists\n",
    "        train_list.append(train)\n",
    "\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = my_train_test_split(rate, filtered_data,n=0)\n",
    "# train_data1, test_data1 = my_train_test_split(rate, filtered_data1,n=0)\n",
    "\n",
    "# print(f\"traing_data query{len(train_data['feature_vector'][0])}\")\n",
    "train_data_new = my_train(rate, filtered_data1)\n",
    "# print(train_data_new[train_data_new['userId'] == 3])\n",
    "\n",
    "# print(\"\\n------------ this is rate ------------\\n\")\n",
    "# user_rate = rate[rate['userId'] == 3]\n",
    "# print(user_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for that specific user\n",
    "# print(type(train_data))\n",
    "# print(train_data)\n",
    "user_id = 2\n",
    "user_data = train_data_new[train_data_new['userId'] == user_id]\n",
    "user_data1 = train_data_new[train_data_new['userId'] == user_id]\n",
    "# user_data1 = test_data[test_data['userId'] == 2]\n",
    "# print(user_data)\n",
    "# print(user_data1)\n",
    "\n",
    "# test_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 5 with Key: Mother (Madeo) (2009) and Movie ID: 76091\n",
      "Mother (Madeo) (2009): Frequency = 5, Movie ID = 76091\n",
      "I, the Jury (1982): Frequency = 4, Movie ID = 6086\n",
      "Villain (1971): Frequency = 4, Movie ID = 136850\n",
      "Thief (1981): Frequency = 3, Movie ID = 5867\n",
      "10th & Wolf (2006): Frequency = 3, Movie ID = 55391\n",
      "9/11 (2002): Frequency = 2, Movie ID = 44943\n",
      "\n",
      " ---------- Next old Filter --------- \n",
      "\n",
      "Maximum value: 5 with Key: Mother (Madeo) (2009) and Movie ID: 76091\n",
      "Mother (Madeo) (2009): Frequency = 5, Movie ID = 76091\n",
      "I, the Jury (1982): Frequency = 4, Movie ID = 6086\n",
      "Villain (1971): Frequency = 4, Movie ID = 136850\n",
      "George Carlin: Jammin' in New York (1992): Frequency = 3, Movie ID = 140265\n",
      "10th & Wolf (2006): Frequency = 3, Movie ID = 55391\n",
      "Thief (1981): Frequency = 3, Movie ID = 5867\n",
      "\n",
      " ---------- Test with Eucl --------- \n",
      "\n",
      "Maximum value: 22 with Key: Mutiny on the Bounty (1962) and Movie ID: 26085\n",
      "Mutiny on the Bounty (1962): Frequency = 22, Movie ID = 26085\n",
      "This Gun for Hire (1942): Frequency = 17, Movie ID = 8765\n",
      "In the Realms of the Unreal (2004): Frequency = 17, Movie ID = 30892\n",
      "I Know Where I'm Going! (1945): Frequency = 16, Movie ID = 4194\n",
      "Chalet Girl (2011): Frequency = 15, Movie ID = 85565\n",
      "Innocents, The (1961): Frequency = 13, Movie ID = 1076\n"
     ]
    }
   ],
   "source": [
    "user_watched_movies = user_data['movieId'].to_numpy()\n",
    "# print(user_watched_movies)\n",
    "# you can use either filtered_data1(new vect) or filtered_data(old vect)\n",
    "movieIDs1 = test_knn(user_data1, filtered_data1, user_watched_movies, k=5)\n",
    "\n",
    "print(\"\\n ---------- Next old Filter --------- \\n\")\n",
    "\n",
    "movieIDs = test_knn(user_data, filtered_data, user_watched_movies, k=5)\n",
    "\n",
    "print(\"\\n ---------- Test with Eucl --------- \\n\")\n",
    "movieIDs2 = test_knn_eucl(user_data1, filtered_data1, user_watched_movies, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Recommended movie: 76091 Similarity:0.39072703062109726\n",
      "Recommended movie: 6086 Similarity:0.4321575608990758\n",
      "Recommended movie: 136850 Similarity:0.4321575608990758\n",
      "Recommended movie: 5867 Similarity:0.4321575608990758\n",
      "Recommended movie: 55391 Similarity:0.4321575608990758\n",
      "Recommended movie: 44943 Similarity:0.09332181348825827\n"
     ]
    }
   ],
   "source": [
    "# Gets the mean of the cosine_similarity between \n",
    "# the user movies and each recommended movies\n",
    "\n",
    "user_test_movies = user_data1['movieId'].to_numpy()\n",
    "print(len(user_test_movies))\n",
    "arr = []\n",
    "\n",
    "for x in movieIDs1:\n",
    "    arr = []\n",
    "    for y in user_test_movies:\n",
    "        z = calculate_cosine_similarity(x, y, filtered_data1)\n",
    "        arr.append(z)\n",
    "\n",
    "    print(f\"Recommended movie: {x} Similarity:{np.mean(arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Recommended movie: 76091 Similarity:0.38524561362207405\n",
      "Recommended movie: 6086 Similarity:0.4260549023385504\n",
      "Recommended movie: 136850 Similarity:0.4260549023385504\n",
      "Recommended movie: 140265 Similarity:0.1899484431762106\n",
      "Recommended movie: 55391 Similarity:0.4260549023385504\n",
      "Recommended movie: 5867 Similarity:0.4260549023385504\n"
     ]
    }
   ],
   "source": [
    "user_test_movies = user_data['movieId'].to_numpy()\n",
    "print(len(user_test_movies))\n",
    "arr = []\n",
    "\n",
    "for x in movieIDs:\n",
    "    arr = []\n",
    "    for y in user_test_movies:\n",
    "        z = calculate_cosine_similarity(x, y, filtered_data)\n",
    "        arr.append(z)\n",
    "\n",
    "    print(f\"Recommended movie: {x} Similarity:{np.mean(arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Recommended movie: 26085 Distance:2.006760311378707\n",
      "Recommended movie: 8765 Distance:2.029227660990688\n",
      "Recommended movie: 30892 Distance:2.0526396520085637\n",
      "Recommended movie: 4194 Distance:2.034655810154132\n",
      "Recommended movie: 85565 Distance:1.9538454467147792\n",
      "Recommended movie: 1076 Distance:1.8786502339049471\n"
     ]
    }
   ],
   "source": [
    "user_test_movies = user_data1['movieId'].to_numpy()\n",
    "print(len(user_test_movies))\n",
    "arr = []\n",
    "\n",
    "for x in movieIDs2:\n",
    "    arr = []\n",
    "    for y in user_test_movies:\n",
    "        z = calculate_euclidean_distance(x, y, filtered_data1)\n",
    "        arr.append(z)\n",
    "\n",
    "    print(f\"Recommended movie: {x} Distance:{np.mean(arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "Precision: 0.96\n",
      "Recall: 0.83\n",
      "True Positives: 135\n",
      "False Positives: 5\n",
      "True Negatives: 1\n",
      "False Negatives: 27\n"
     ]
    }
   ],
   "source": [
    "def evaluate_recommendations(user_data, recommended_movie_ids, filtered_data, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Evaluates recommendation accuracy using cosine similarity and user ratings.\n",
    "    For movies that have a rating of 4 or 5 by the user we will want our \n",
    "    recommendation to have a cosine_similarity between recommended and that \n",
    "    movie, to be above the cosine_similarity threshold we defined. if we have a \n",
    "    user rated movie below 2, but we have a the cosine_similarity above the \n",
    "    threshold that will be our false positive.\n",
    "\n",
    "    Args:\n",
    "        user_data (DataFrame): Data for a single user, with columns 'movieId' and 'rating'.\n",
    "        recommended_movie_ids (list): List of movieIds recommended to the user.\n",
    "        filtered_data (DataFrame): Full dataset with feature vectors for movies.\n",
    "        similarity_threshold (float): The cosine similarity threshold.\n",
    "\n",
    "    Returns:\n",
    "        dict: Accuracy, precision, recall, and other metrics.\n",
    "    \"\"\"\n",
    "    # User's watched movies and their ratings\n",
    "    user_test_movies = user_data['movieId'].to_numpy()\n",
    "    user_ratings = user_data.set_index('movieId')['rating'].to_dict()\n",
    "\n",
    "    # Initialize metrics\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for recommended_movie in recommended_movie_ids:\n",
    "        # Calculate similarities for all user test movies\n",
    "        similarities = [\n",
    "            calculate_cosine_similarity(recommended_movie, test_movie, filtered_data)\n",
    "            for test_movie in user_test_movies\n",
    "        ]\n",
    "\n",
    "        # Use the mean similarity for this recommendation\n",
    "        mean_similarity = np.mean(similarities)\n",
    "\n",
    "        # Check if the recommended movie is similar above the threshold\n",
    "        is_similar = mean_similarity >= similarity_threshold\n",
    "\n",
    "        # Determine if this is TP, FP, TN, or FN\n",
    "        for test_movie in user_test_movies:\n",
    "            user_rating = user_ratings[test_movie]\n",
    "            # print(f\"{test_movie}, {user_rating}\")\n",
    "\n",
    "            if user_rating >= 3:  # User likes the movie\n",
    "                if is_similar:\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    false_negatives += 1\n",
    "            elif user_rating <= 2:  # User dislikes the movie\n",
    "                if is_similar:\n",
    "                    false_positives += 1\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    total = true_positives + false_positives + true_negatives + false_negatives\n",
    "    accuracy = (true_positives + true_negatives) / total if total > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"true_negatives\": true_negatives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "user_test_movies = user_data1['movieId'].to_numpy()\n",
    "recommended_movie_ids = movieIDs1  # Replace with your list of recommended movies\n",
    "similarity_threshold = 0.3\n",
    "\n",
    "metrics = evaluate_recommendations(user_data1, recommended_movie_ids, filtered_data1, similarity_threshold)\n",
    "\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2f}\")\n",
    "print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "print(f\"True Positives: {metrics['true_positives']}\")\n",
    "print(f\"False Positives: {metrics['false_positives']}\")\n",
    "print(f\"True Negatives: {metrics['true_negatives']}\")\n",
    "print(f\"False Negatives: {metrics['false_negatives']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.003239149761515"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_euclidean_distance(29, 32, filtered_data1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
